# -*- coding: utf-8 -*-
"""finalmlp_hw1_q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eZ0bKPs-YBVMO8aZ3r9w8oolGUGz2FUn

![picture]https://drive.google.com/file/d/1xUjvWktf5ueJDmeDUdc7KWNcYwn1WgA8/view?usp=sharing

Please check the above link for the forward pass diagram
"""

import torch
import math
from math import log
device = torch.device('cpu')

class MLP:
    def __init__(
        self,
        linear_1_in_features,
        linear_1_out_features,
        f_function,
        linear_2_in_features,
        linear_2_out_features,
        g_function
    ):
        """
        Check the activation functions and return corresponding non-linearity
        if f_function == 'relu':
            h_act = ReLU(h)
        elif f_function == 'Sigmoid':
            h_act = Sigmoid(h)
        else:
            h_act == Identity(h)
        """
        self.f_function = f_function
        print(self.f_function)
        print(type(self.f_function))
        self.g_function = g_function

        self.parameters = dict(
            W1 = torch.randn(linear_1_out_features, linear_1_in_features),
            b1 = torch.randn(linear_1_out_features),
            W2 = torch.randn(linear_2_out_features, linear_2_in_features),
            b2 = torch.randn(linear_2_out_features),
        )
        self.grads = dict(
            dJdW1 = torch.zeros(linear_1_out_features, linear_1_in_features),
            dJdb1 = torch.zeros(linear_1_out_features),
            dJdW2 = torch.zeros(linear_2_out_features, linear_2_in_features),
            dJdb2 = torch.zeros(linear_2_out_features),
        )

        # put all the cache value you need in self.cache
        self.cache = dict()
        
    def forward(self, x):
        
        def ReLU(x):
            return x * (x > 0)
        def Sigmoid(x):
            eps = 0.00001
            return 1/1+(torch.exp(-x))+eps
        def Identity(x):
            return x
        
        h = x.mm(self.parameters['W1'].T) + self.parameters['b1'] #z2
        print("H fn" , h.shape) #(10x20)
        if self.f_function == 'relu':
            f_act = ReLU(h)
        elif self.f_function == 'sigmoid':
            f_act = Sigmoid(h)
        elif self.f_function == 'identity':
            f_act == Identity(h)
        else:
            print("No function defined for f_function")
        print("f_act fn" , f_act.shape)
        
        g = f_act.mm(self.parameters[ 'W2'].T) + self.parameters['b2']  
        
        print("g.shape", g.shape) #(10x1)
        if self.g_function == 'relu':
            g_act = ReLU(g)
        elif self.g_function == 'sigmoid':
            print("Enter")
            g_act = Sigmoid(g)
        elif self.g_function == 'identity':
            g_act = Identity(g)
        else:
            #g_act = None
            print("No function defined for g_function")
        
        #print("g = ", g)
        

        #h_f = h.clamp(min = 0)
        print("g_act.shape", g_act.shape)
        #The output of the activation function of the g layer itself is the yhat
        y_pred = g_act #yhat
        print(y_pred.shape) #(10x1)
        return y_pred
        #pass

    def backward(self, dJdy_hat):
        """
        Args:
            dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)
        """
        '''
        Each activation function's gradient is defined and called twice(g and h in backward pass)
        '''
        def dReLU(x):
            return x * (x > 0)
        def dSigmoid(x):
            eps = 0.00001
            return (torch.exp(-x))/(1+(torch.exp(-x))).pow(2)+eps
        def dIdentity(x):
            return 1

        # Implement the backward function
        l_grad = 1.0 #dL/dL
        g = dJdy_hat 
        print(dJdy_hat)
        if self.g_function == 'relu':
            g_grad = dReLU(g)
        elif self.g_function == 'sigmoid':
            g_grad = dSigmoid(g)
        elif self.g_function == 'identity':
            g_grad = dIdentity(g)
        else:
            print("No function defined for g_function")
            
            
        #each function's gradient is calculated as the product of the local gradient and the global gradient as shown in the diagram(in pdf)
        self.grads[ 'dJdz3'] = g_grad*(dJdy_hat)
        z3_grad = g_grad.mm(dJdy_hat.T) #dyp/dz3 probably g_funcgrad
        
        print("grad.z3 ", z3_grad)
        z2_grad = self.parameters['W2'] #dz_3/dz_2
        print("grad.z2 ", z2_grad)
        #z1_grad = 1.0
        #print("grad.z1 ", z1_grad)  
        
        
        h = x.mm(self.parameters['W1'].T) + self.parameters['b1']
        if self.f_function == 'relu':
            h_grad = dReLU(h)
        elif self.f_function == 'sigmoid':
            h_grad = dSigmoid(h)
        elif self.f_function == 'identity':
            h_grad = dIdentity(h)
        else:
            print("No function defined for f_function")
            
        z1_grad = self.parameters['W1']
        print("grad.z1 ", z1_grad)     
        self.grads['dJdW2'] = h_grad.t().mm(dJdy_hat) #w2.grad                   
        #w2_grad = h_grad.t().mm(y_hat_grad)
        
        #grad_h = grad_h_relu.clone()
        grad_h = self.grads[ 'dJdW2'].clone() #lin1.grad
        #w1_grad = x.t().mm(grad_h) 
        self.grads['dJdW1'] = x.t().mm(dJdy_hat).mm(self.parameters[ 'W2']) #w2.grad
        self.grads['dJdb1'] = torch.eye(self.parameters['b1'].shape[0])
        self.grads['dJdb2'] = torch.eye(self.parameters['b2'].shape[0])
        print("b2.grad", self.grads['dJdb2'])
        print("b1.grad", self.grads['dJdb1'])
        #return w2_grad
        pass


    def clear_grad_and_cache(self):
        for grad in self.grads:
            self.grads[grad].zero_()
        self.cache = dict()

def mse_loss(y, y_hat):
    """
    Args:
        y: the label tensor (batch_size, linear_2_out_features)
        y_hat: the prediction tensor (batch_size, linear_2_out_features)

    Return:
        J: scalar of loss
        dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)
    """
    # Implement the mse loss
    y_hat_grad = y_hat - y #yhat.grad()
    loss = 0.5*(y_hat - y).pow(2)
    print("MSE Loss is = ", loss )
    return loss, y_hat_grad
    pass

def bce_loss(y, y_hat):
    """
    Args:
        y_hat: the prediction tensor
        y: the label tensor
        
    Return:
        loss: scalar of loss
        dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)
    """
    # Implement the bce loss
    y_hat_grad = (1-y)/(1-y_hat) - (y/y_hat) #yhat.grad()
    loss1 = (y*torch.log(y_hat))
    print(loss1)
    loss2 = ((1-y)*torch.log(1-y_hat))
    print(loss2)
    loss = -(loss1 + loss2)
#    loss = -[(y*torch.log(y_hat)) + ((1-y)*torch.log(1-y_hat)) ]
    return loss, y_hat_grad
    pass

from collections import OrderedDict

import torch
import torch.nn as nn
import torch.nn.functional as F 

net = MLP(
    linear_1_in_features=1,
    linear_1_out_features=10,
    f_function='sigmoid',
    linear_2_in_features=10,
    linear_2_out_features=1,
    g_function='relu'
)

x = torch.randn(10, 1)
y = ((torch.randn(10) > 0.5) * 1.0).unsqueeze(-1)

y

net.clear_grad_and_cache() #0-grad

y_hat = net.forward(x)

y_hat

J, dJdy_hat = mse_loss(y, y_hat)

J

dJdy_hat

net.clear_grad_and_cache() # 0-grad

net.backward(dJdy_hat)